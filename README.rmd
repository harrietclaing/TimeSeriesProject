---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "How do oil price changes impact economic variables in the period 1990 to 2017: A Replication of the Cologni & Manera paper"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Harriet Catherine Laing"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University, Stellenbosch, South Africa" # First Author's Affiliation
Email1: "21617023\\@sun.ac.za" # First Author's Email address

#Author2: "John Smith"
#Ref2: "Some other Institution, Cape Town, South Africa"
#Email2: "John\\@gmail.com"
#CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

#Author3: "John Doe"
#Email3: "Joe\\@gmail.com"

#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
#keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
#addtoprule: TRUE
#addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
#Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
#link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
#abstract: |
 # Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 4, fig.align='center', fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(zoo)
library(tseries)
library(vars)
library(urca)
library(AICcmodavg)
library(ggplot2)
library(ggfortify)
library(pracma)
library(readr)
library(Hmisc)
library(wordcountaddin)
library(Texevier)
library(rmarkdown)
library(grid)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
library(tsDyn)
library(ggplotify)
library(grid)
wordcountaddin::word_count("README.Rmd")

ExchangeRate <- read_csv("data/ExchangeRate.csv")
GlobalPriceofBrentCrudeUSDollars <- read_csv("data/GlobalPriceofBrentCrudeUSDollars.csv")
Inflation <- read_csv("data/Inflation.csv")
InterestRate <- read_csv("data/InterestRate.csv")
MonetaryAggregate <- read_csv("data/MonetaryAggregate.csv")
RealGDP <- read_csv("data/RealGDP.csv")

#convert monthly to quarterly & change
 

WorldOilPricedollars <- GlobalPriceofBrentCrudeUSDollars %>% 
                        group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q"))%>%
                        summarise_all(mean) %>% 
                        filter(DATE>"1979Q4" & DATE<"2017Q1")

WorldOilPrice_dollars <- data.frame(DATE=WorldOilPricedollars$DATE, WorldOilPrice=WorldOilPricedollars$POILBREUSDM)

#rename column in dataframe
InterestRateUS_ <- rename(InterestRate, InterestRate=FEDFUNDS)

#convert monthly to quarterly

InterestRates_US <- InterestRateUS_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

#rename column in dataframe
InflationUS_ <- rename(Inflation, Inflation=CPIAUCSL)

#convert monthly to quarterly

Inflation_US <- InflationUS_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1") #one period back to get a change later on
#we want to filter to get the correct time frame

#rename column in dataframe
MonetaryAggregateM1US_ <- rename(MonetaryAggregate, MonetaryAggregate=MYAGM1USM052S)

#convert monthly to quarterly

MonetaryAggregateM1_US <- MonetaryAggregateM1US_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

#rename column in dataframe
ExchangeRatesOECDUS_ <- rename(ExchangeRate, ExchangeRate=FGSDRSQ027S)

#convert monthly to quarterly

ExchangeRates_US <- ExchangeRatesOECDUS_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

#rename column in dataframe
RealGDPUS_ <- rename(RealGDP, RealGDP=NGDPRSAXDCUSQ)

RealGDP_US <- RealGDPUS_ %>% 
                group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
                filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

logRealGDP_US <- data.frame(DATE=RealGDP_US$DATE, logRealGDP=log(RealGDP_US$RealGDP))
logExchangeRates_US <-data.frame(DATE=ExchangeRates_US$DATE, logExchangeRates=log(ExchangeRates_US$ExchangeRate))
logWorldOilPrice <- data.frame(DATE=WorldOilPrice_dollars$DATE, logWorldOilPrice=log(WorldOilPrice_dollars$WorldOilPrice))
logInflation_US <- data.frame(DATE=Inflation_US$DATE, logInflation=log(Inflation_US$Inflation))
logMonetaryAggregateM1_US <- data.frame(DATE=MonetaryAggregateM1_US$DATE, logMonetaryAggregateM1_US=log(MonetaryAggregateM1_US$MonetaryAggregate))

dfMonetaryAggregate_Inflation <- data.frame(DATE=MonetaryAggregateM1_US$DATE, logInflation=logInflation_US$logInflation, logMonetaryAggregateM1= logMonetaryAggregateM1_US$logMonetaryAggregate)

TransformedMonetaryAggregateM1_US <- mutate(dfMonetaryAggregate_Inflation, TransformedMonetaryAggregateM1 = logMonetaryAggregateM1-logInflation)

TransformedInflation_US <- mutate(logInflation_US, TransformedInflation = logInflation_US$logInflation - lag(logInflation_US$logInflation, default=first(logInflation_US$logInflation)))
#0 for the first period

DetrendedlogRealGDP <- detrend(logRealGDP_US$logRealGDP)
DetrendedTransformedMonetary <- detrend(TransformedMonetaryAggregateM1_US$TransformedMonetaryAggregateM1)

interestrate <- ts(InterestRates_US$InterestRate, start=c(1990,1), end=c(2016, 4), freq=4)
inflation <- ts(Inflation_US$Inflation, start=c(1990,1), end=c(2016, 4), freq=4)
realGDP <- ts(logRealGDP_US$logRealGDP, start=c(1990,1), end=c(2016, 4), freq=4)
exchangerate <- ts(logExchangeRates_US$logExchangeRates, start=c(1990,1), end=c(2016, 4), freq=4)
worldoilprice <- ts(logWorldOilPrice$logWorldOilPrice, start=c(1990,1), end=c(2016, 4), freq=4)
monetaryaggregate <- ts(TransformedMonetaryAggregateM1_US$TransformedMonetaryAggregateM1, start=c(1990,1), end=c(2016, 4), freq=4)

# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
#write_rds(Dataset_Loan_amounts, path = "data/Dataset_Loan_amounts.xlsx")
```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

To replicate the study by Cologni & Manera, the same methodology and structural cointegrated VAR model was used. This paper investigates whether the findings from Cologni & Manera hold after 2003 in the United States, as the economic impact of a rise in oil prices during the period 1990 to 2017 is analysed. This paper sought to find whether the response of central banks to a sudden oil price shock reduce the impact thereof.

This replication paper finds that...is the optimal model for this time period and ...

# Background

Many economists regard increases in the oil price as a major cause of asymmetries in the business cycle. Many studies have found a negative impact on aggregate economic activity as a result of an increase in the oil price (Hamilton, 1983). This issue became especially important to economists following the world oil market highs in the early 2000s which could lead to economic slowdowns in developed countries. The hypothesised asymmetric relationship between oil prices and economic activity was first hypothesised after the oil price collapse of 1986 which did not lead to an economic boom, which is what theoretically should have been the case following the view that there existed an inverse relationship between oil prices and the economy.

However, the channels through which an increase in the oil price impacts economic variables remains unclear. There are many possible theoretical explanations; some of which link to the effect of decreasing firms' profits which may reduce capital spending and consumers' expectations which causes them to consume more today, others link to the income transfer between oil-importing countries and oil exporting countries that shifts, and others link an increase in oil prices to the increases in prices of related goods, thereby increasing inflation.

There is also, importantly, effects on economic variables that flow indirectly from an increase in oil prices. Namely, that the monetary authority responds to increase employment and ensure price stability once an increase in oil prices threatens these two objectives. For example, in the US, the Federal bank ...
The role of monetary policy may cause delayed impacts of an increase in oil prices on economic variables.


# Replication methodology & results


## Finding the data

As per the paper, the data was sourced where possible from the IMF. However, for the interest rate and exchange rate we sourced the data from Board of Governors of the Federal Reserve System and for inflation from the US Bureau of Labor Statistics. First, we want to use US data and see if we can replicate the results in the study. We need to convert all the data into quarterly. The time period available to reproduce the results of the paper was constrained by the availability of world oil price data: could only find from 1990, therefore had to limit to 1990 onwards. Similarly, restricted time period due to the data availability of money. In the paper they used predominantly seasonally-adjusted data but due to constraints on availability of data, for this replication I used a combination of seasonally-adjusted and not seasonally-adjusted.

```{r table1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl1 <- "
| Economic variable     | Cologni & Manera (all IMF)               | Replication paper 
|-----------------------|:----------------------------------------:|:----------------------------------------------:|
|Interest Rate          |Federal Funds rate (% per annum)          |Federal Funds Effective rate (% per month)      |
|Inflation              |Consumer price index (index number)       |Consumer price index (1982-1984=100)            | 
|Real GDP               |Real GDP (constant prices 1995) (billions)|Real GDP (constant prices 1995) (billions) (IMF)| 
|Monetary Aggregate     |Money M1 (billions)                       |Money M1 (billions) (IMF)                       |
|Exchange Rate          |US dollars (per SDR)                      |US dollars (millions)                           |
|World Oil Price        |International average price (Brent dated) |US dollars per barrel (IMF)                     |
"
cat(tabl1)
```

The methodology that I applied in order to replicate the study, was to first transform all of the quarterly series in logarithms except for the interest rate. As in the paper, we run Augmented Dickey Fuller tests on all the time series variables. Findings at the 1% confidence interval were that are all variables were non-stationary and were integrated of order 1, except for the monetary aggregate which was found to be integrated of order 2. The lags were selected according to the AIC criteria, as done in the paper. The results in this replication differed only from the paper regarding the integration order of inflation, which was found to be integrated of order 1 by Cologni & Manera. This difference from the paper dictated that only the monetary aggregate be transformed by subtracting inflation to become the real monetary aggregate (by taking the difference between the logarithm of monetary aggregate and the logarithm of inflation), and the transformation for inflation was not followed. This is because for finding cointegrating relationships, the time series variables must be integrated of order 1.

```{r, echo = FALSE}
g<- ggplot(InterestRates_US) +
    geom_line(aes(DATE,InterestRate, group=1), size=0.8, colour='blue') +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Interest rates") +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())

h<- ggplot(logInflation_US) +
    geom_line(aes(DATE,logInflation, group=1), size=0.8, colour='red') +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Inflation") +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())

i<- ggplot(logRealGDP_US) +
    geom_line(aes(DATE,logRealGDP, group=1), size=0.8, colour='orange') +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Real GDP") +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())

k<- ggplot(TransformedMonetaryAggregateM1_US) +
    geom_line(aes(DATE,TransformedMonetaryAggregateM1, group=1), size=0.8, colour='purple') +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Monetary Aggregate") +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())

l<- ggplot(logExchangeRates_US) +
    geom_line(aes(DATE,logExchangeRates, group=1), size=0.8, colour='pink') +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Exchange Rate") +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())


m<- ggplot(logWorldOilPrice) +
    geom_line(aes(DATE,logWorldOilPrice, group=1), size=0.8, colour='green') +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "World Oil Price") +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())

grid.arrange(g, h, i, k, l, m, main = "Figure 1: Replicated time series variables from 1980-2017")
```
We then construct our VAR model and estimate the number of lags to be included. According to the paper, the lag max is set to 4 and the AIC lag selection criteria is used. The VAR model in the replication was found to have 4 lags and accounted for a time trend, as is done in the paper. We order our VAR system in the same way as the short run restrictions matrix in the paper: monetary aggregate, interest rate, real GDP, inflation, exchange rate and world oil price. Cologni & Manera find that the number of lags should be 2. As can be seen in Figure 1, there is clearly a lot of persistence after the financial crisis. Exchange rates for the US had a stark level increase around this water-shed event and interest rates were set close to zero to try and stimulate the economy, where they have remained fairly constant since this monetary policy adjustment. Similarly, we can note real GDP has diminished since 2008. Therefore, it is likely the difference in the optimal lags between this replication and the Cologni & Manera paper arises from the different time periods used, as Cologni & Manera's time period ends before the financial crisis.

For a VAR model, we use the lag of 4. For the VECM model, we use the lag of 3 because a VECM model has a difference term.

```{r, echo = FALSE}
groupedVAR <- cbind(monetaryaggregate, interestrate, realGDP, inflation, exchangerate, worldoilprice)
colnames(groupedVAR) <- cbind("MonetaryAggregate", "InterestRate", "RealGDP", "Inflation", "ExchangeRate", "WorldOilPrice")
#ordered according to B matrix

cointanalysisVAR <- cbind(realGDP, worldoilprice, interestrate, inflation, exchangerate, monetaryaggregate)
#ordered according to table 3
```
```{r, echo = FALSE}
#lagselect <- VARselect(groupedVAR, lag.max=4, type="trend")
#lagselect$selection
```

Now we can see long-run trends in the time series variables, but wish to now see if there exists any cointegrating relationships. We test this using the Johansen test, namely the eigenvalue test and the trace test. For the eigenvalue test, we find we cannot reject the null hypothesis that the number of cointegrating relationships is between 0 and 1 at the 5% confidence interval where our critical value is smaller than the test statistic, however, only marginally (37.26 estimated value < 37.52 critical value). For the trace test, we find that we cannot reject the null hypothesis that the number of cointegrating relationships is between 1 and 2. Therefore, because the rejection of the null hypothesis in the eigenvalue test is incredibly marginal, we conclude from our estimates that there is likely two cointegrating relationships. This is a divergence from the result as is found for the US in Cologni & Manera.

Cointegration analysis of the restricted system

We obtain the cointegrating vectors from the Johansen test and construct a matrix is which each column is a cointegrating vector. Then we multiply the VAR system by the cointegrating vector matrix to obtain the error correction terms.
 
```{r, echo = FALSE}
Johansentestcointanalysis <- ca.jo(cointanalysisVAR, type="eigen", K=2, ecdet="trend", spec="longrun")

VECModelNew <- VECM(cointanalysisVAR,lag=3, r=2, estim="ML")

VARModelNew <- VAR(cointanalysisVAR, p=2, type = "trend", lag.max = 4)
```

# Restricted VAR system

The covariance matrix of residuals from the VAR system provides us with an estimate of the contemporaneous effects of each variable.

Let us check which variables might be cointegrated, by regressing them on each other and checking if the residuals are stationary or not

monetaryaggregate, interestrate, realGDP, inflation, exchangerate, worldoilprice
```{r}
gold_eq <- lm(worldoilprice ~ interestrate, data=cointanalysisVAR)
summary(gold_eq)

gold_eq$residuals %>%
  ur.df(., lags = 4, type = "none") %>%
  summary()
```
At 1%...
M & IR not stationary, M& G not, & inf not, &exch not, &oil not
int rate and oil YES (yes)
IR & GDP yes (yes)
IR & inflation yes (yes)
IR & exch rate yes (no)
GDP & exch no, & infla no, %oil no
infla & exch no
infla & oil price yes (no)
exh & oil no

We want to set up our own VECM model
```{r}
dat_ecm <- tibble(
  gold_d = diff(dat$gold)[-1],
  silver_d = diff(dat$silver)[-1],
  error_ecm1 = gold_eq$residuals[-1:-2],
  gold_d1 = diff(dat$gold)[-(length(dat$gold) - 1)],
  silver_d1 = diff(dat$silver)[-(length(dat$silver) - 1)]
)
```

Let us set up the imposed restrictions in a matrix, known as B matrix in the paper.
```{r, echo = FALSE}
Brow1 <- c(1,1,0,1,0,0)
Brow2 <- c(0,1,1,1,1,1)
Brow3 <- c(0,0,1,1,1,1)
Brow4 <- c(0,0,0,1,1,1)
Brow5 <- c(0,0,0,0,1,1)
Brow6 <- c(0,0,0,0,0,1)
#m, r, y, p, e, o


B_Matrix <- rbind(Brow1, Brow2, Brow3, Brow4, Brow5, Brow6)

colnames(B_Matrix)<-NULL
rownames(B_Matrix) <- NULL
```

We want to find whether our restrictions are exogenous or not.
```{r}
#H2 <- alrtest(Johansentestcointanalysis, A = B_Matrix, r = 1)
#summary(H2)

#DA <- matrix(c(1,0,0,0), c(4,1))
#summary(alrtest(sjd.vecm, A=DA, r=1))
```


According to the paper, $u_t=E_tB$ which means that we can recover the short run error vector $u_t$ from the long-run error we obtain from our VAR system. We obtain the covariance matrix of residuals from our VAR system and then mutiply that by our B matrix of short-run restrictions and the transpose of the B matrix, because the paper states that these are equivalent. 
```{r, echo = FALSE}
VARModelRestricted <- VAR(groupedVAR, p=2, type = "trend", lag.max = 4)

covariancematrix <- summary(VARModelRestricted)$covres
```



```{r, echo = FALSE}
covariancematrix*B_Matrix*t(B_Matrix)
```
We then obtain a single entry for each row, which we can create our short-run error vector from.
```{r}
ErrorVector_U <- c(0.0001328802, 0.07138726, 2.804883e-05, 0.753182, 0.01135468,  0.02108534)
```

Comparison of cointegrating vectors
```{r, echo = FALSE}
cointegratingvector1 <- c(1, 0, -0.1704922, -0.01710960, 0.6161332, -0.3747751)
cointegratingvector2 <- c(0,1, 1.6056301, 0.06991751, -5.4962156, 6.5786487)
papercointegratingvector<- c(1, 0.16, 0, -26.019, 0.218, 0)

cointegratinganalysis <- rbind(papercointegratingvector, cointegratingvector1, cointegratingvector2)
#realGDP, worldoilprice, interestrate, inflation, exchangerate, monetaryaggregate
```

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
|                       | Real GDP | World Oil Price | Interest Rate | Inflation | Exchange Rate | Monetary Aggregate |
|-----------------------|:--------:|:---------------:|:-------------:|:---------:|:-------------:|:------------------:|
|Cologni & Manera       | 1        | 0.16            | 0             | -26.019   | 0.218         | 0                  |
|Cointegrating vector 1 | 1        | 0               | -0.170        | -0.017    | 0.616         | -0.375             |
|Cointegrating vector 2 | 0        | 1               | 1.606         | 0.0699    | -5.496        | 6.579              |
"
cat(tabl)
```


```{r, echo = FALSE}
VECModel <- ca.jo(groupedVAR, type="eigen", K=2, ecdet="trend", spec="longrun") #incl linear trend as per paper
VARModel <- vec2var(VECModel, r=2)
```

```{r}
#summary(VARModel)
```

We need to find the SR error, which is equal to the LR error (ECT in the VECM) multiplied by the SR restrictions (B matrix)
```{r, echo=FALSE}

#ECT1 <- c(-0.0025, -1.4757, -1.2205, -1.8461, -1.9858, -0.0757)
#ECT2<- c(-0.0002, -0.1583, -0.1277, -0.0425, -0.1907, -0.0145)

#ECT1*B_Matrix
#ECT2*B_Matrix

#GET COVARIANCE OF ECTS???

#LogLikelihood <- logLik(VECModel)
#LogLikelihoodVAR <- logLik(VARModel)
#?logLik
```



```{r, echo = FALSE}
M_IRF <- irf(VARModel, impulse = "WorldOilPrice", response = "MonetaryAggregate",
             n.ahead = 8, ortho = FALSE, runs = 1000)
IR_IRF <- irf(VARModel, impulse = "WorldOilPrice", response = "InterestRate",
             n.ahead = 8, ortho = FALSE, runs = 1000)
GDP_IRF <- irf(VARModel, impulse = "WorldOilPrice", response = "RealGDP",
             n.ahead = 8, ortho = FALSE, runs = 1000)
I_IRF <- irf(VARModel, impulse = "WorldOilPrice", response = "Inflation",
             n.ahead = 8, ortho = FALSE, runs = 1000)
ER_IRF <- irf(VARModel, impulse = "WorldOilPrice", response = "ExchangeRate",
             n.ahead = 8, ortho = FALSE, runs = 1000)
M <- plot(M_IRF)
I <- plot(IR_IRF)
G <- plot(GDP_IRF)
In <- plot (I_IRF)
E <- plot(ER_IRF)

#grid.arrange(M, I, G, In, E)
```


```{r, echo = FALSE}
#jotestEigen_US <- ca.jo(groupedVAR, type="eigen", K=2, ecdet="trend", spec="longrun")
#cointegrating_vectors <- jotestEigen_US@V

#jotestTrace_US <- ca.jo(groupedVAR, type="trace", K=2, ecdet="trend", spec="longrun") #incl linear trend as per paper
# to get loading coeffs
#cointegrating_loadingcoeffs <- jotestEigen_US@W

#remove the trend line and titles from the matrices so that it is 6x6
#cointegrating_vectors_ <- cointegrating_vectors[-7,-7]
#cointegrating_loadingcoeffs_ <- cointegrating_loadingcoeffs[-7,-7]
    
#colnames(cointegrating_vectors_)<-NULL
#rownames(cointegrating_vectors_) <- NULL

#colnames(cointegrating_loadingcoeffs_)<-NULL
#rownames(cointegrating_loadingcoeffs_) <- NULL

# to get cointegrating matrix pi we need to multiply the cointegrating vector matrix B transposed by the cointegrating loading coefficients matrix alpha
#cointegrating_matrix <- t(cointegrating_vectors_)*cointegrating_loadingcoeffs_
```

Cointegrating vectors visually??
```{r, echo=FALSE}
#plot(cointegrating_vectors)
```




...Let us see if we can impose the restrictions contained in the B matrix onto the cointegrating vectors contained in the Et matrix.
```{r, echo = FALSE}
#U_Matrix <- cointegrating_matrix*B_Matrix
#U_Matrix
```


```{r, echo = FALSE}
VECModel <- VECM(cointanalysisVAR, lag=3, r=2, estim="ML")
summary(VECModel)
```
#Find VECM residuals



```{r, echo = FALSE}
#coeffs <- summary(VECModel)$coefMat

#ect_coeffs <- coeffs[grep("ECT", rownames(coeffs)),]
#now we have a matrix of all of the ECT and variables
```
```{r, echo = FALSE}
#need covariance of ecm terms in matrix 
#cov(ect_coeffs)
```
We want to compare our estimates to the one in Table 3. 

Maybe instead it is the VAR system ...missing constant and trend
```{r, echo=FALSE}
#set up table to compare cointegrating analysis of restricted system
cointvectorr1 <- c(1, 0, -0.04625888, -0.01065431, 0.3009478, 0.03289175)
cointvectorr2 <- c(0,1,0.43236715,0.01061212,-2.4453864,2.64998507)
cointvectorpaper <- c(1, 0.16, 0, -26.019, 0.218, 0, -8.511, -0.009)

cointanalysiscompare <- rbind(cointvectorpaper, cointvectorr1, cointvectorr2)
    
colnames(cointanalysiscompare)<-cbind("y", "o", "r", "p", "e", "m", "c", "trend")
#rownames(cointanalysiscompare) <- NULL

#kable(cointanalysiscompare, caption = "Cointegration analysis of restricted system")
```




Then test for white noise residuals.



```{r, echo = FALSE}
#groupedVARendogmatrix <- as.matrix(groupedVARendog)
#error_correction_terms <- cointegrating_vectors*groupedVARendogmatrix #matrix has headings...
```


```{r, echo = FALSE}
#adf.InterestRate <- ur.df(InterestRates_US$InterestRate, type="none", selectlags = c("AIC"))
#summary(adf.InterestRate) #non-stat
```



# Step 6: Is it stationary?

# Step 7: Are the residuals white noise?

# Step 8: Find VECM model by imposing SR contemporaneous effects

# Step 9: Test model specification using congruency, parsimony, lag inclusion...

Alternatively, exclude exchange rates as the paper unusually included these. However, in our models we could not find exchange rates to be significant.

# Appendix {-}


```{r, echo = FALSE}
#summary(jotestEigen_US)
```

```{r, echo = FALSE}
#summary(jotestTrace_US)
```


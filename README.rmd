---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "How do oil price changes impact economic variables in the period 1990 to 2017: A Replication of the Cologni & Manera paper"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Nico Katzke}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Harriet Catherine Laing"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University, Stellenbosch, South Africa" # First Author's Affiliation
Email1: "21617023\\@sun.ac.za" # First Author's Email address

#Author2: "John Smith"
#Ref2: "Some other Institution, Cape Town, South Africa"
#Email2: "John\\@gmail.com"
#CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

#Author3: "John Doe"
#Email3: "Joe\\@gmail.com"

#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
#keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
#addtoprule: TRUE
#addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
#Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
#link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
#abstract: |
 # Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 4, fig.align='center', fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(zoo)
library(tseries)
library(vars)
library(urca)
library(AICcmodavg)
library(ggplot2)
library(ggfortify)
library(pracma)
library(readr)
library(Hmisc)
library(wordcountaddin)
library(Texevier)
library(rmarkdown)
library(grid)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
library(tsDyn)
wordcountaddin::word_count("README.Rmd")

ExchangeRate <- read_csv("data/ExchangeRate.csv")
GlobalPriceofBrentCrudeUSDollars <- read_csv("data/GlobalPriceofBrentCrudeUSDollars.csv")
Inflation <- read_csv("data/Inflation.csv")
InterestRate <- read_csv("data/InterestRate.csv")
MonetaryAggregate <- read_csv("data/MonetaryAggregate.csv")
RealGDP <- read_csv("data/RealGDP.csv")

#convert monthly to quarterly & change
 

WorldOilPricedollars <- GlobalPriceofBrentCrudeUSDollars %>% 
                        group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q"))%>%
                        summarise_all(mean) %>% 
                        filter(DATE>"1979Q4" & DATE<"2017Q1")

WorldOilPrice_dollars <- data.frame(DATE=WorldOilPricedollars$DATE, WorldOilPrice=WorldOilPricedollars$POILBREUSDM)

#rename column in dataframe
InterestRateUS_ <- rename(InterestRate, InterestRate=FEDFUNDS)

#convert monthly to quarterly

InterestRates_US <- InterestRateUS_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

#rename column in dataframe
InflationUS_ <- rename(Inflation, Inflation=CPIAUCSL)

#convert monthly to quarterly

Inflation_US <- InflationUS_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1") #one period back to get a change later on
#we want to filter to get the correct time frame

#rename column in dataframe
MonetaryAggregateM1US_ <- rename(MonetaryAggregate, MonetaryAggregate=MYAGM1USM052S)

#convert monthly to quarterly

MonetaryAggregateM1_US <- MonetaryAggregateM1US_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

#rename column in dataframe
ExchangeRatesOECDUS_ <- rename(ExchangeRate, ExchangeRate=FGSDRSQ027S)

#convert monthly to quarterly

ExchangeRates_US <- ExchangeRatesOECDUS_ %>% 
    group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
    summarise_all(mean) %>% 
    filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

#rename column in dataframe
RealGDPUS_ <- rename(RealGDP, RealGDP=NGDPRSAXDCUSQ)

RealGDP_US <- RealGDPUS_ %>% 
                group_by(DATE = format(as.yearqtr(DATE, "%b-%Y"), "%YQ%q")) %>%
                filter(DATE>"1989Q4" & DATE<"2017Q1")
#we want to filter to get the correct time frame

logRealGDP_US <- data.frame(DATE=RealGDP_US$DATE, logRealGDP=log(RealGDP_US$RealGDP))
logExchangeRates_US <-data.frame(DATE=ExchangeRates_US$DATE, logExchangeRates=log(ExchangeRates_US$ExchangeRate))
logWorldOilPrice <- data.frame(DATE=WorldOilPrice_dollars$DATE, logWorldOilPrice=log(WorldOilPrice_dollars$WorldOilPrice))
logInflation_US <- data.frame(DATE=Inflation_US$DATE, logInflation=log(Inflation_US$Inflation))
logMonetaryAggregateM1_US <- data.frame(DATE=MonetaryAggregateM1_US$DATE, logMonetaryAggregateM1_US=log(MonetaryAggregateM1_US$MonetaryAggregate))

dfMonetaryAggregate_Inflation <- data.frame(DATE=MonetaryAggregateM1_US$DATE, logInflation=logInflation_US$logInflation, logMonetaryAggregateM1= logMonetaryAggregateM1_US$logMonetaryAggregate)

TransformedMonetaryAggregateM1_US <- mutate(dfMonetaryAggregate_Inflation, TransformedMonetaryAggregateM1 = logMonetaryAggregateM1-logInflation)

TransformedInflation_US <- mutate(logInflation_US, TransformedInflation = logInflation_US$logInflation - lag(logInflation_US$logInflation, default=first(logInflation_US$logInflation)))
#0 for the first period

DetrendedlogRealGDP <- detrend(logRealGDP_US$logRealGDP)
DetrendedTransformedMonetary <- detrend(TransformedMonetaryAggregateM1_US$TransformedMonetaryAggregateM1)

interestrate <- ts(InterestRates_US$InterestRate, start=c(1990,1), end=c(2016, 4), freq=4)
inflation <- ts(Inflation_US$Inflation, start=c(1990,1), end=c(2016, 4), freq=4)
realGDP <- ts(logRealGDP_US$logRealGDP, start=c(1990,1), end=c(2016, 4), freq=4)
exchangerate <- ts(logExchangeRates_US$logExchangeRates, start=c(1990,1), end=c(2016, 4), freq=4)
worldoilprice <- ts(logWorldOilPrice$logWorldOilPrice, start=c(1990,1), end=c(2016, 4), freq=4)
monetaryaggregate <- ts(TransformedMonetaryAggregateM1_US$TransformedMonetaryAggregateM1, start=c(1990,1), end=c(2016, 4), freq=4)

# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
#write_rds(Dataset_Loan_amounts, path = "data/Dataset_Loan_amounts.xlsx")
```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

To replicate the study by Cologni & Manera to find the economic impact of a rise in oil prices.

# Background

# Replication methodology & results


# Step 1: Find the data

As per the paper, the data was sourced where possible from the IMF. However, for the interest rate and exchange rate we sourced the data from Board of Governors of the Federal Reserve System and for inflation from the US Bureau of Labor Statistics. First, we want to use US data and see if we can replicate the results in the study. We need to convert all the data into quarterly. The time period available to reproduce the results of the paper was constrained by the availability of world oil price data: could only find from 1990, therefore had to limit to 1990 onwards. Similarly, restricted time period due to the data availability of money. In the paper they used predominantly seasonally-adjusted data but due to constraints on availability of data, for this replication I used a combination of seasonally-adjusted and not seasonally-adjusted.

Interest Rate= Federal Funds Effective Rate, percent, not seasonally adjusted, monthly Source:  Board of Governors of the Federal Reserve System
Exchange Rate= Millions of Dollars, Not Seasonally Adjusted, quarterly, source: Board of Governors of the Federal Reserve System (2021)
Inflation = Index 1982-1984=100, Seasonally Adjusted, monthly, source  U.S. Bureau of Labor Statistics
Real GDP = Domestic Currency, Seasonally Adjusted, quarterly, source IMF
Monetary Aggregate = Dollars, Seasonally Adjusted, monthly source: IMF
World Oil Price = U.S. Dollars per Barrel, Not Seasonally Adjusted, monthly, IMF. Need to do until 2017 because of monetary aggregate data constraints

The methodology that I applied in order to replicate the study, was to first transform all of the quarterly series in logarithms except for the interest rate. As in the paper, we run Augmented Dickey Fuller tests on all the time series variables. Findings at the 1% confidence interval were that are all variables were non-stationary and were integrated of order 1, except for the monetary aggregate which was found to be integrated of order 2. The lags were selected according to the AIC criteria, as done in the paper. The results in this replication differed only from the paper regarding the integration order of inflation, which was found to be integrated of order 1 by Cologni & Manera. This difference from the paper dictated that only the monetary aggregate be transformed by subtracting inflation to become the real monetary aggregate (by taking the difference between the logarithm of monetary aggregate and the logarithm of inflation), and the transformation for inflation was not followed. This is because for finding cointegrating relationships, the time series variables must be integrated of order 1.

Resultant time series variables were as follows:

```{r}
autoplot <- autoplot(cbind(interestrate, inflation, realGDP, exchangerate, worldoilprice, monetaryaggregate), label=NULL)
autoplot
```
```{r}
g<- ggplot() +
    geom_point(aes(InterestRates_US$DATE, InterestRates_US$InterestRate)) +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Interest rates") +
    theme(axis.text.x = element_blank())

h<- ggplot() +
    geom_point(aes(logInflation_US$DATE, logInflation_US$logInflation))+
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Inflation") +
    theme(axis.text.x = element_blank())

i<- ggplot() +
    geom_point(aes(logRealGDP_US$DATE, logRealGDP_US$logRealGDP))+
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Real GDP") +
    theme(axis.text.x = element_blank())

k<- ggplot() +
    geom_point(aes(TransformedMonetaryAggregateM1_US$DATE, TransformedMonetaryAggregateM1_US$TransformedMonetaryAggregateM1))+
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Monetary Aggregate") +
    theme(axis.text.x = element_blank())

l<- ggplot() +
    geom_point(aes(logRealGDP_US$DATE, logExchangeRates_US$logExchangeRates)) +
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "Real GDP") +
    theme(axis.text.x = element_blank())

m<- ggplot() +
    geom_point(aes(logWorldOilPrice$DATE, logWorldOilPrice$logWorldOilPrice))+
    xlab("") +
    ylab("") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title = "World Oil Price") +
    theme(axis.text.x = element_blank())



grid.arrange(g, h, i, k, l, m)
```
We then construct our VAR model by creating a matrix which includes the time series variables included in Figure 1. We set the lag max to 4. The VAR model was found to have 3 lags (4 - 1) when we used the AIC lag selection criteria and accounted for a time trend, as is done in the paper.

```{r}
groupedVAR <- cbind(monetaryaggregate, interestrate, realGDP, inflation, exchangerate, worldoilprice)
colnames(groupedVAR) <- cbind("MonetaryAggregate", "InterestRate", "RealGDP", "Inflation", "ExchangeRate", "WorldOilPrice")
```
```{r}
lagselect <- VARselect(groupedVAR, lag.max=4, type="trend")
lagselect$selection

#paper uses AIC criteria...lag - 1
```

Use AIC and find that lag should be 2 according to the paper, but we find AIC suggests 4, so we use 3. As can be seen in Figure 1, there is clearly a lot of persistence after the financial crisis. Exchange rates for the US had a stark level increase around this water-shed event and interest rates were set close to zero to try and stimulate the economy, where they have remained fairly constant since this monetary policy adjustment. Similarly, we can note real GDP has diminished since 2008.

Now we can see long-run trends in the time series variables, but wish to now see if there exists any cointegrating relationships. We test this using the Johansen test, namely the eigenvalue test and the trace test. For the eigenvalue test, we find that we can reject the null hypothesis of the number of cointegrating relationships equalling 3 or exceeding it at the 5% confidence interval where our critical value is smaller than the test statistic. For the trace test, we find that we can reject the null hypothesis when the relationships equal or exceed 2. Therefore, because the rejection of the null hypothesis in the eigenvalue test is incredibly marginal (25.71 critical value marginally larger than 25.54 test statistic), we conclude from our estimates that there is likely one cointegrating relationship. This is the same result as is found for the US in Cologni & Manera.

We obtain the cointegrating vectors from the Johansen test and construct a matrix is which each column is a cointegrating vector. Then we multiply the VAR system by the cointegrating vector matrix to obtain the error correction terms.
 
```{r}
cointegrating_vectors <- jotestEigen_US@V

# to get loading coeffs
cointegrating_loadingcoeffs <- jotestEigen_US@W

#remove the trend line and titles from the matrices so that it is 6x6
cointegrating_vectors_ <- cointegrating_vectors[-7,-7]
cointegrating_loadingcoeffs_ <- cointegrating_loadingcoeffs[-7,-7]
    
colnames(cointegrating_vectors_)<-NULL
rownames(cointegrating_vectors_) <- NULL

colnames(cointegrating_loadingcoeffs_)<-NULL
rownames(cointegrating_loadingcoeffs_) <- NULL

# to get cointegrating matrix pi
cointegratingmatrix <- cointegrating_vectors_*cointegrating_loadingcoeffs_
```
Let us set up the imposed restrictions in a matrix, known as B matrix in the paper.
```{r}

Brow1 <- c(1,1,0,1,0,0)
Brow2 <- c(0,1,1,1,1,1)
Brow3 <- c(0,0,1,1,1,1)
Brow4 <- c(0,0,0,1,1,1)
Brow5 <- c(0,0,0,0,1,1)
Brow6 <- c(0,0,0,0,0,1)

B_Matrix <- rbind(Brow1, Brow2, Brow3, Brow4, Brow5, Brow6)
B_Matrix

colnames(B_Matrix)<-NULL
rownames(B_Matrix) <- NULL
```
Let us see if we can impose the restrictions contained in the B matrix onto the cointegrating vectors contained in the Et matrix.
```{r}
U_Matrix <- cointegrating_matrix*B_Matrix
U_Matrix
```
# Set up VECM

```{r}
VECModel <- VECM(groupedVAR, lag=3, r=1, estim="ML")
summary(VECModel)
```
```{r}
coeffs <- summary(VECModel)$coefMat

ect_coeffs <- coeffs[ "ECR","Estimate"]
#now we have a matrix of all of the ECT and variables
```
```{r}
#need covariance of ecm terms in matrix 
cov(ect_coeffs)
```




Then test for white noise residuals.



```{r}
#groupedVARendogmatrix <- as.matrix(groupedVARendog)
#error_correction_terms <- cointegrating_vectors*groupedVARendogmatrix #matrix has headings...
```


# Step 3: Augemented Dickey-Fuller tests

The null-hypothesis of an Augmented Dickey-Fuller test is that the series has a unit-root. We ask, is the estimated critical value small enough to reject the null-hypothesis? If yes, we cannot reject the null hypothesis, therefore, the series may be non-stationary.

In this section, we find that the interest rate is stationary, change in inflation is stationary, detrended real GDP is still not stationary, detrended monetary aggregate is still non-stationary and world oil price is non stationary.

# Augmented Dickey Fuller tests

```{r}
adf.InterestRate <- ur.df(InterestRates_US$InterestRate, type="none", selectlags = c("AIC"))
summary(adf.InterestRate) #non-stat

adf.difInterestRate <- ur.df(diff(InterestRates_US$InterestRate), type="none", selectlags =c("AIC"))
summary(adf.difInterestRate) #stat
```

```{r}
adf.Inflation <- ur.df(logInflation_US$logInflation, type = "none", selectlags = c("AIC"))
summary(adf.Inflation) #non stat

adf.difInflation <- ur.df(diff(logInflation_US$logInflation), type = "none", selectlags = c("AIC"))
summary(adf.difInflation) #stat
```

Real GDP

```{r}
adf.RealGDP <- ur.df(logRealGDP_US$logRealGDP, type = "none", selectlags = c("AIC"))
summary(adf.RealGDP) #non stat

adf.difRealGDP <- ur.df(diff(logRealGDP_US$logRealGDP), type = "none", selectlags = c("AIC"))
summary(adf.difRealGDP) #stat
```
Monetary Aggregates

```{r}
adf.MonetaryAggregate <- ur.df(logMonetaryAggregateM1_US$logMonetaryAggregate, type = "none", selectlags = c("AIC"))
summary(adf.MonetaryAggregate) #non-stat

adf.difMonetaryAggregate <- ur.df(diff(logMonetaryAggregateM1_US$logMonetaryAggregate), type = "none", selectlags = c("AIC"))
summary(adf.difMonetaryAggregate) #non-stat
```
Exchange Rate

```{r}
adf.ExchangeRates <- ur.df(logExchangeRates_US$logExchangeRates, type = "none", selectlags = c("AIC"))
summary(adf.ExchangeRates)#non stat

adf.difExchangeRates <- ur.df(diff(logExchangeRates_US$logExchangeRates), type = "none", selectlags = c("AIC"))
summary(adf.difExchangeRates) #stat
```

World Oil Price
```{r}
adf.WorldOilPrice <- ur.df(logWorldOilPrice$logWorldOilPrice, type = "none", selectlags = c("AIC"))
summary(adf.WorldOilPrice) #non stat
```

# Set up the VAR model

# Build VAR model
```{r, echo=TRUE}
ModelUS <- VAR(groupedVAR, p=3, type="trend", season=NULL, exog=NULL)
summary(ModelUS)
```





# Step 5: ACF & PACF? or AIC criteria to choose lags

```{r}

```


# Step 6: Is it stationary?

# Step 7: Are the residuals white noise?
```{r}
#ljungbox test
```


# Step 8: Find VECM model by imposing SR contemporaneous effects

# Step 9: Test model specification using congruency, parsimony, lag inclusion...

# Appendix {-}

# Johansen Tests {-}
```{r}
jotestEigen_US <- ca.jo(groupedVAR, type="eigen", K=2, ecdet="trend", spec="longrun") #incl linear trend as per paper
summary(jotestEigen_US)
```

```{r}
jotestTrace_US <- ca.jo(groupedVAR, type="trace", K=2, ecdet="trend", spec="longrun") #incl linear trend as per paper
summary(jotestTrace_US)
```

